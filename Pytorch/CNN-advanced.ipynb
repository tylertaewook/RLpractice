{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Advanced (EMNIST dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the comment below only when using Google Colab\n",
    "# !pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Colab에서 실행할 경우 위 주석을 해제하고 실행하면 됨. Colab에는 pytorch가 설치되어있지 않으므로 설치 필요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os, sys\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow, imsave\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'CNN'\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMNIST(Dataset):\n",
    "    '''\n",
    "    EMNIST Dataset (split: letters)\n",
    "    Reference:\n",
    "        Dataset: https://www.kaggle.com/crawford/emnist\n",
    "        Paper: https://arxiv.org/abs/1702.05373v1\n",
    "    '''\n",
    "    url = {\n",
    "        'train': 'https://www.dropbox.com/s/pnfxiohzpq75i6a/emnist-train.npy?dl=1',\n",
    "        'test': 'https://www.dropbox.com/s/2sl1kzcemnh2qlj/emnist-test.npy?dl=1',\n",
    "    }\n",
    "    \n",
    "    def __init__(self, data_path, transform, download=True, train=True):\n",
    "        '''\n",
    "        Args:\n",
    "            data_path (str): path to dataset\n",
    "        '''\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.mode = 'train' if train else 'test'\n",
    "        if download:\n",
    "            fpath = self._download(self.url[self.mode], data_path)\n",
    "        else:\n",
    "            fpath = os.path.join(data_path, 'emnist-{}.npy'.format(self.mode))\n",
    "        _file = np.load(fpath)\n",
    "        self.images = _file[:, 1:].reshape([len(_file), 28, 28])\n",
    "        self.labels = torch.tensor(_file[:, 0]-1, dtype=torch.long)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        imgs = self.transform(Image.fromarray(self.images[idx]))\n",
    "        return imgs, self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def _download(self, url, data_path):\n",
    "        data = urllib.request.urlopen(url)\n",
    "        filename = url.rpartition('/')[-1][:-5]\n",
    "        if not os.path.exists(data_path):\n",
    "            os.makedirs(data_path)\n",
    "        file_path = os.path.join(data_path, filename)\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(data.read())\n",
    "        print('Downloaded: ', file_path)\n",
    "        return file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchvision의 MNIST dataset과 비슷한 form으로 만들었음.\n",
    "\n",
    "torchvision의 EMNIST가 제대로 작동하지 않아서 직접 만듦."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DragonNet(nn.Module):\n",
    "    \"\"\"\n",
    "        Simple CNN Clssifier\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(DragonNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # (N, 1, 28, 28)\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(3, 2, padding=1),\n",
    "            # (N, 32, 14, 14)\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # (N, 64, 7, 7)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(7*7*64, 32),\n",
    "            nn.Dropout(p=0.8),\n",
    "            nn.Linear(32, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_ = self.conv(x) # (N, 64, 7, 7)\n",
    "        y_ = y_.view(y_.size(0), -1) # (N, 64*7*7)\n",
    "        y_ = self.fc(y_)\n",
    "        return y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nn.Conv2d**: Conv2d 사용방법 및 다양한 모듈 class [Doc](https://pytorch.org/docs/stable/nn.html#conv2d)\n",
    "\n",
    "모델에 개선의 여지를 많이 남겨 두었음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = DragonNet(num_classes=26).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),    \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5),std=(0.5, 0.5, 0.5))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=(0.5, 0.5, 0.5),\n",
    "                                std=(0.5, 0.5, 0.5))]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**transforms**: 다양한 transform 함수 [Doc](https://pytorch.org/docs/stable/torchvision/transforms.html). Test transform은 변경 X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = EMNIST(data_path='data', download=True, train=True, transform=transform_train)\n",
    "test_set = EMNIST(data_path='data', download=True, train=False, transform=transform_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 정의한 EMNIST class를 통해 dataset 생성\n",
    "\n",
    "폴더에 없을 시에 download하고, 앞서 정의한 transform에 따라 전처리 된 데이터를 return함.\n",
    "\n",
    "train은 trainset, testset에 대한 return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=100, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_epoch = 3\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "scheduler = StepLR(optim, step_size=1000, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/3, Step: 0, Loss: 3.4183974266052246, lr:0.001\n",
      "******************** Test ********************\n",
      "Step: 0, Loss: 4.401900768280029, Accuracy: 5.405405405405405 %\n",
      "**********************************************\n",
      "Epoch: 0/3, Step: 500, Loss: 2.5385336875915527, lr:0.001\n",
      "Epoch: 0/3, Step: 1000, Loss: 2.4766345024108887, lr:0.00095\n",
      "******************** Test ********************\n",
      "Step: 1000, Loss: 1.5496608018875122, Accuracy: 52.66216216216216 %\n",
      "**********************************************\n",
      "Epoch: 0/3, Step: 1500, Loss: 2.2788522243499756, lr:0.00095\n",
      "Epoch: 0/3, Step: 2000, Loss: 2.1016623973846436, lr:0.0009025\n",
      "******************** Test ********************\n",
      "Step: 2000, Loss: 1.0034339427947998, Accuracy: 65.01351351351352 %\n",
      "**********************************************\n",
      "Epoch: 0/3, Step: 2500, Loss: 1.5291451215744019, lr:0.0009025\n",
      "Epoch: 1/3, Step: 3000, Loss: 1.859971284866333, lr:0.000857375\n",
      "******************** Test ********************\n",
      "Step: 3000, Loss: 0.9616425037384033, Accuracy: 67.87837837837839 %\n",
      "**********************************************\n",
      "Epoch: 1/3, Step: 3500, Loss: 2.0776143074035645, lr:0.000857375\n",
      "Epoch: 1/3, Step: 4000, Loss: 1.9020872116088867, lr:0.0008145062499999999\n",
      "******************** Test ********************\n",
      "Step: 4000, Loss: 0.9082263112068176, Accuracy: 67.27702702702703 %\n",
      "**********************************************\n",
      "Epoch: 1/3, Step: 4500, Loss: 1.6463371515274048, lr:0.0008145062499999999\n",
      "Epoch: 1/3, Step: 5000, Loss: 1.2730286121368408, lr:0.0007737809374999998\n",
      "******************** Test ********************\n",
      "Step: 5000, Loss: 0.7519301772117615, Accuracy: 71.16216216216216 %\n",
      "**********************************************\n",
      "Epoch: 1/3, Step: 5500, Loss: 1.4649313688278198, lr:0.0007737809374999998\n",
      "Epoch: 2/3, Step: 6000, Loss: 1.5076512098312378, lr:0.0007350918906249999\n",
      "******************** Test ********************\n",
      "Step: 6000, Loss: 0.7306068539619446, Accuracy: 72.85810810810811 %\n",
      "**********************************************\n",
      "Epoch: 2/3, Step: 6500, Loss: 1.3297966718673706, lr:0.0007350918906249999\n",
      "Epoch: 2/3, Step: 7000, Loss: 1.7449123859405518, lr:0.0006983372960937497\n",
      "******************** Test ********************\n",
      "Step: 7000, Loss: 0.5857755541801453, Accuracy: 73.11486486486487 %\n",
      "**********************************************\n",
      "Epoch: 2/3, Step: 7500, Loss: 2.012073278427124, lr:0.0006983372960937497\n",
      "Epoch: 2/3, Step: 8000, Loss: 1.6451644897460938, lr:0.0006634204312890623\n",
      "******************** Test ********************\n",
      "Step: 8000, Loss: 0.5986208319664001, Accuracy: 73.01351351351352 %\n",
      "**********************************************\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epoch):\n",
    "    for idx, (images, labels) in enumerate(train_loader):\n",
    "        # Training Discriminator\n",
    "        x, y = images.to(DEVICE), labels.to(DEVICE) # (N, 1, 28, 28), (N, )\n",
    "        y_hat = model(x) # (N, 10)\n",
    "        \n",
    "        loss = criterion(y_hat, y)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        if step % 500 == 0:\n",
    "            print('Epoch: {}/{}, Step: {}, Loss: {}, lr:{}'.format(\n",
    "                epoch, max_epoch, step, loss.item(), scheduler.get_lr()[0]))\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            model.eval()\n",
    "            acc = 0.\n",
    "            with torch.no_grad():\n",
    "                for idx, (images, labels) in enumerate(test_loader):\n",
    "                    x, y = images.to(DEVICE), labels.to(DEVICE) # (N, 1, 28, 28), (N, )\n",
    "                    y_hat = model(x) # (N, 10)\n",
    "                    loss = criterion(y_hat, y)\n",
    "                    _, indices = torch.max(y_hat, dim=-1)\n",
    "                    acc += torch.sum(indices == y).item()\n",
    "            print('*'*20, 'Test', '*'*20)\n",
    "            print('Step: {}, Loss: {}, Accuracy: {} %'.format(step, loss.item(), acc/len(test_set)*100))\n",
    "            print('*'*46)\n",
    "            model.train()\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Test ********************\n",
      "Step: 8325, Loss: 0.6139542460441589, Accuracy: 75.38513513513514 %\n",
      "**********************************************\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "model.eval()\n",
    "acc = 0.\n",
    "with torch.no_grad():\n",
    "    for idx, (images, labels) in enumerate(test_loader):\n",
    "        x, y = images.to(DEVICE), labels.to(DEVICE) # (N, 1, 28, 28), (N, )\n",
    "        y_hat = model(x) # (N, 10)\n",
    "        loss = criterion(y_hat, y)\n",
    "        _, indices = torch.max(y_hat, dim=-1)\n",
    "        acc += torch.sum(indices == y).item()\n",
    "print('*'*20, 'Test', '*'*20)\n",
    "print('Step: {}, Loss: {}, Accuracy: {} %'.format(step, loss.item(), acc/len(test_set)*100))\n",
    "print('*'*46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 28]), tensor(1))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 888 # 0 to 9999\n",
    "img, y = test_set[idx]\n",
    "img.shape, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2c9036b62b0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlB/IFOUHMkX5gUxRfiBTlB/IFOUHMvX/viZ/eNQSS4cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2c9020150f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(img[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측은 B\n",
      "정답은 B\n"
     ]
    }
   ],
   "source": [
    "sample = img.unsqueeze(dim=0).to(DEVICE)\n",
    "out = model(sample)\n",
    "_, idx = out.max(dim=-1)\n",
    "print('예측은 {}'.format(chr(idx.item()+65)))\n",
    "print('정답은 {}'.format(chr(y+65)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
